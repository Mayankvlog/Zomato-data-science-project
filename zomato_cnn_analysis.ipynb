{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6783b3c9",
   "metadata": {},
   "source": [
    "# Zomato Data Analysis using Advanced CNN Neural Network\n",
    "\n",
    "This project implements an advanced Convolutional Neural Network (CNN) with 5 hidden layers to analyze the Zomato dataset. We'll use various Python libraries including TensorFlow, Keras, PySpark, and visualization tools to build and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7866a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pickle\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9931d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ZomatoAnalysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data using pandas\n",
    "df = pd.read_csv('data/zomato.csv')\n",
    "\n",
    "# Convert to PySpark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Number of rows: {spark_df.count()}\")\n",
    "print(f\"Number of columns: {len(spark_df.columns)}\")\n",
    "print(\"\\nColumn Names:\")\n",
    "print(spark_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d429f",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Let's analyze our dataset through various visualizations to understand the patterns and relationships between different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40628eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plotting style\n",
    "plt.style.use('seaborn')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# 1. Distribution of ratings\n",
    "sns.histplot(data=df, x='rating', bins=20, ax=axes[0,0])\n",
    "axes[0,0].set_title('Distribution of Ratings')\n",
    "\n",
    "# 2. Average cost for two vs ratings\n",
    "sns.scatterplot(data=df, x='rating', y='average_cost_for_two', ax=axes[0,1])\n",
    "axes[0,1].set_title('Average Cost vs Ratings')\n",
    "\n",
    "# 3. Cuisine distribution (top 10)\n",
    "cuisine_counts = df['cuisines'].value_counts().head(10)\n",
    "sns.barplot(x=cuisine_counts.values, y=cuisine_counts.index, ax=axes[1,0])\n",
    "axes[1,0].set_title('Top 10 Cuisines')\n",
    "\n",
    "# 4. Online delivery availability\n",
    "delivery_counts = df['has_online_delivery'].value_counts()\n",
    "sns.pieplot(delivery_counts.values, labels=delivery_counts.index, ax=axes[1,1])\n",
    "axes[1,1].set_title('Online Delivery Availability')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc2a232",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Feature Engineering\n",
    "\n",
    "Now we'll prepare our data for the CNN model by:\n",
    "1. Handling missing values\n",
    "2. Encoding categorical variables\n",
    "3. Scaling numerical features\n",
    "4. Reshaping data for CNN input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae27cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Encode categorical variables using PySpark\n",
    "categorical_columns = ['cuisines', 'location', 'rest_type', 'type']\n",
    "for col in categorical_columns:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=f\"{col}_index\")\n",
    "    encoder = OneHotEncoder(inputCols=[f\"{col}_index\"], outputCols=[f\"{col}_encoded\"])\n",
    "    spark_df = indexer.fit(spark_df).transform(spark_df)\n",
    "    spark_df = encoder.fit(spark_df).transform(spark_df)\n",
    "\n",
    "# Convert back to pandas for further processing\n",
    "df_processed = spark_df.toPandas()\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = ['votes', 'average_cost_for_two']\n",
    "df_processed[numerical_columns] = scaler.fit_transform(df_processed[numerical_columns])\n",
    "\n",
    "# Prepare features for CNN\n",
    "X = df_processed.drop(['rating'], axis=1)\n",
    "y = df_processed['rating']\n",
    "\n",
    "# Reshape data for CNN (samples, timesteps, features)\n",
    "X = X.values.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Testing data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b584f4a4",
   "metadata": {},
   "source": [
    "# CNN Model Architecture\n",
    "\n",
    "We'll create a CNN model with 5 hidden layers using different activation functions:\n",
    "1. Conv1D layer with ReLU activation\n",
    "2. Dense layer with tanh activation\n",
    "3. Dense layer with sigmoid activation\n",
    "4. Dense layer with softmax activation\n",
    "5. Dense layer with linear activation\n",
    "\n",
    "The model will use binary cross-entropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "def create_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        # 1. Conv1D layer with ReLU activation\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        # Flatten layer to connect Conv1D to Dense layers\n",
    "        Flatten(),\n",
    "        \n",
    "        # 2. Dense layer with tanh activation\n",
    "        Dense(128, activation='tanh'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # 3. Dense layer with sigmoid activation\n",
    "        Dense(64, activation='sigmoid'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # 4. Dense layer with softmax activation\n",
    "        Dense(32, activation='softmax'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # 5. Dense layer with linear activation\n",
    "        Dense(16, activation='linear'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_cnn_model(input_shape=(X_train.shape[1], 1))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ff9df0",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "We'll train the model with:\n",
    "- Early stopping to prevent overfitting\n",
    "- Model checkpoint to save the best model\n",
    "- Batch size of 32\n",
    "- 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1daecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'model/best_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df1675",
   "metadata": {},
   "source": [
    "# Model Evaluation and Predictions\n",
    "\n",
    "Let's evaluate our model's performance on the test set and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809c09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "test_loss, test_mae, test_mse = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(test_mse):.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Ratings')\n",
    "plt.ylabel('Predicted Ratings')\n",
    "plt.title('Actual vs Predicted Ratings')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate additional metrics\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "print(\"\\nAdditional Metrics:\")\n",
    "print(f\"R² Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3359d4f3",
   "metadata": {},
   "source": [
    "# Save Model and Scaler\n",
    "\n",
    "Finally, we'll save our trained model and scaler for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee48f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('model/zomato_cnn_model.h5')\n",
    "\n",
    "# Save the scaler\n",
    "with open('model/zomato_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Model and scaler saved successfully!\")\n",
    "\n",
    "# Function to load the model and make predictions\n",
    "def load_model_and_predict(data):\n",
    "    \"\"\"\n",
    "    Load the saved model and scaler to make predictions on new data\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    loaded_model = tf.keras.models.load_model('model/zomato_cnn_model.h5')\n",
    "    \n",
    "    # Load the scaler\n",
    "    with open('model/zomato_scaler.pkl', 'rb') as f:\n",
    "        loaded_scaler = pickle.load(f)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    scaled_data = loaded_scaler.transform(data[numerical_columns])\n",
    "    \n",
    "    # Reshape for CNN\n",
    "    reshaped_data = scaled_data.reshape(scaled_data.shape[0], scaled_data.shape[1], 1)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = loaded_model.predict(reshaped_data)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "print(\"Example usage of the prediction function:\")\n",
    "sample_data = df[numerical_columns].head(1)\n",
    "print(\"\\nSample input:\")\n",
    "print(sample_data)\n",
    "print(\"\\nPredicted rating:\", load_model_and_predict(sample_data)[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
